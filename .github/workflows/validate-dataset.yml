name: Dataset Quality Validation

on:
  workflow_run:
    workflows: ["Build Master Dataset"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Dataset build run ID to validate'
        required: false
        type: string

jobs:
  validate:
    runs-on: ubuntu-latest
    # Only run on successful builds
    if: |
      github.event_name == 'workflow_dispatch' || 
      github.event.workflow_run.conclusion == 'success'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Determine run ID
        id: run_id
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "run_id=${{ inputs.run_id }}" >> $GITHUB_OUTPUT
          else
            echo "run_id=${{ github.event.workflow_run.id }}" >> $GITHUB_OUTPUT
          fi
      
      - name: Download dataset artifact
        uses: actions/download-artifact@v4
        with:
          name: master-dataset
          run-id: ${{ steps.run_id.outputs.run_id }}
          path: data/master-dataset
          github-token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Setup DuckDB
        run: |
          wget -q https://github.com/duckdb/duckdb/releases/download/v1.1.3/duckdb_cli-linux-amd64.zip
          unzip -q duckdb_cli-linux-amd64.zip
          chmod +x duckdb
          ./duckdb --version
      
      - name: Extract quality metrics
        id: metrics
        run: |
          ./duckdb <<'EOF' > metrics.json
          INSTALL parquet;
          LOAD parquet;
          
          -- Core metrics extraction
          COPY (
            SELECT 
              (SELECT COUNT(*) FROM 'data/master-dataset/channels.parquet') as total_channels,
              (SELECT COUNT(*) FROM 'data/master-dataset/streams.parquet') as total_streams,
              (SELECT COUNT(*) FROM 'data/master-dataset/programs.parquet') as total_programs,
              (SELECT ROUND(AVG(quality_score), 3) FROM 'data/master-dataset/channels.parquet') as avg_quality_score,
              (SELECT COUNT(*) FROM 'data/master-dataset/channels.parquet' WHERE quality_score < 0.5) as low_quality_channels,
              (SELECT COUNT(DISTINCT channel_id) FROM 'data/master-dataset/streams.parquet') as channels_with_streams,
              (SELECT COUNT(DISTINCT country) FROM 'data/master-dataset/channels.parquet') as countries,
              (SELECT COUNT(DISTINCT category) FROM 'data/master-dataset/channels.parquet' WHERE category IS NOT NULL) as categories
          ) TO STDOUT (FORMAT JSON, ARRAY true);
          EOF
          
          echo "ðŸ“Š Quality Metrics:"
          cat metrics.json | jq .
          
          # Export for next steps
          echo "metrics<<EOF" >> $GITHUB_OUTPUT
          cat metrics.json >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Validate quality thresholds
        id: validation
        run: |
          python3 <<'PYTHON'
          import json
          import sys
          from pathlib import Path
          
          # Load metrics
          metrics_file = Path('metrics.json')
          with metrics_file.open() as f:
              metrics = json.load(f)[0]
          
          print("\nðŸ” Validating quality thresholds...\n")
          
          # Define thresholds (adjust based on your baseline)
          THRESHOLDS = {
              'total_channels': {
                  'min': 1000,
                  'description': 'Minimum expected channels'
              },
              'avg_quality_score': {
                  'min': 0.6,
                  'description': 'Minimum average quality score'
              },
              'low_quality_channels': {
                  'max': 200,
                  'description': 'Maximum channels with quality < 0.5'
              },
              'total_streams': {
                  'min': 500,
                  'description': 'Minimum expected streams'
              }
          }
          
          failures = []
          warnings = []
          
          # Validate each threshold
          for metric, config in THRESHOLDS.items():
              actual = metrics.get(metric, 0)
              
              if 'min' in config:
                  threshold = config['min']
                  if actual < threshold:
                      failures.append(
                          f"âŒ {metric}: {actual} < {threshold} ({config['description']})"
                      )
                  else:
                      print(f"âœ… {metric}: {actual} >= {threshold}")
              
              elif 'max' in config:
                  threshold = config['max']
                  if actual > threshold:
                      failures.append(
                          f"âŒ {metric}: {actual} > {threshold} ({config['description']})"
                      )
                  else:
                      print(f"âœ… {metric}: {actual} <= {threshold}")
          
          # Additional warnings (non-blocking)
          if metrics['channels_with_streams'] < metrics['total_channels'] * 0.8:
              warnings.append(
                  f"âš ï¸ Only {metrics['channels_with_streams']}/{metrics['total_channels']} "
                  f"channels have streams ({metrics['channels_with_streams']/metrics['total_channels']*100:.1f}%)"
              )
          
          # Print summary
          print("\n" + "="*60)
          if failures:
              print("\nðŸš¨ VALIDATION FAILED\n")
              for failure in failures:
                  print(failure)
              sys.exit(1)
          else:
              print("\nâœ… ALL QUALITY CHECKS PASSED\n")
              
          if warnings:
              print("âš ï¸ Warnings (non-blocking):\n")
              for warning in warnings:
                  print(warning)
          
          print("="*60)
          PYTHON
      
      - name: Generate quality report
        if: always()
        run: |
          ./duckdb <<'EOF' > quality_report.md
          INSTALL parquet;
          LOAD parquet;
          
          .mode markdown
          
          SELECT '# Dataset Quality Report' as report_title;
          SELECT '' as empty_line;
          SELECT '## Summary' as section_header;
          SELECT '' as empty_line;
          
          -- Summary metrics
          SELECT 
            'Total Channels' as metric,
            COUNT(*)::VARCHAR as value
          FROM 'data/master-dataset/channels.parquet'
          UNION ALL
          SELECT 
            'Total Streams',
            COUNT(*)::VARCHAR
          FROM 'data/master-dataset/streams.parquet'
          UNION ALL
          SELECT 
            'Total Programs',
            COUNT(*)::VARCHAR
          FROM 'data/master-dataset/programs.parquet'
          UNION ALL
          SELECT
            'Avg Quality Score',
            ROUND(AVG(quality_score), 3)::VARCHAR
          FROM 'data/master-dataset/channels.parquet';
          
          SELECT '' as empty_line;
          SELECT '## Top 10 Countries by Channel Count' as section_header;
          SELECT '' as empty_line;
          
          SELECT 
            country,
            COUNT(*) as channels
          FROM 'data/master-dataset/channels.parquet'
          GROUP BY country
          ORDER BY channels DESC
          LIMIT 10;
          
          SELECT '' as empty_line;
          SELECT '## Quality Score Distribution' as section_header;
          SELECT '' as empty_line;
          
          SELECT 
            CASE 
              WHEN quality_score >= 0.8 THEN 'Excellent (â‰¥0.8)'
              WHEN quality_score >= 0.6 THEN 'Good (0.6-0.8)'
              WHEN quality_score >= 0.4 THEN 'Fair (0.4-0.6)'
              ELSE 'Poor (<0.4)'
            END as quality_tier,
            COUNT(*) as channels,
            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1)::VARCHAR || '%' as percentage
          FROM 'data/master-dataset/channels.parquet'
          GROUP BY quality_tier
          ORDER BY MIN(quality_score) DESC;
          EOF
          
          echo "ðŸ“„ Quality Report:"
          cat quality_report.md
      
      - name: Upload quality report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-report-${{ steps.run_id.outputs.run_id }}
          path: |
            metrics.json
            quality_report.md
          retention-days: 30
      
      - name: Create issue on validation failure
        if: failure() && github.event_name != 'workflow_dispatch'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read metrics if available
            let metricsTable = '';
            try {
              const metrics = JSON.parse(fs.readFileSync('metrics.json', 'utf8'))[0];
              metricsTable = `\n## Metrics\n\n| Metric | Value |\n|--------|-------|\n`;
              for (const [key, value] of Object.entries(metrics)) {
                metricsTable += `| ${key} | ${value} |\n`;
              }
            } catch (e) {
              metricsTable = '\n_Metrics not available_\n';
            }
            
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸš¨ Dataset Quality Validation Failed - Build #${{ steps.run_id.outputs.run_id }}`,
              body: `Dataset quality validation failed for build run [${{ steps.run_id.outputs.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ steps.run_id.outputs.run_id }}).
              
              ${metricsTable}
              
              ## Action Required
              
              1. Review the [validation workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
              2. Check the [dataset build logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ steps.run_id.outputs.run_id }})
              3. Investigate root cause:
                 - API failures (iptv-org, IPTVPortal)
                 - Schema changes
                 - Scraper breakage
              4. Fix and re-run build-dataset.yml
              
              ## Auto-assigned
              
              /cc @${{ github.repository_owner }}
              
              ---
              _This issue was automatically created by the quality validation workflow._`,
              labels: ['bug', 'data-quality', 'automated']
            });
            
            console.log(`Created issue #${issue.data.number}`);
      
      - name: Post summary
        if: always()
        run: |
          echo "## Dataset Quality Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f quality_report.md ]; then
            cat quality_report.md >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“¦ [Download full report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
